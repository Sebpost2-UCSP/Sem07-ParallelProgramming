{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install build-essential\n",
        "!apt-get install -y libcudnn8 libcudnn8-dev\n",
        "!apt-get install g++"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3JTcW9UKD5q",
        "outputId": "ca56a824-d981-4603-a815-fe461eda9559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Connecting to security.ubuntu.com] [1 InRele\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Connecting to security.ubuntu.com] [Connecti\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [46.8 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,467 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,241 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,194 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,420 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,150 kB]\n",
            "Fetched 7,878 kB in 3s (2,789 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following held packages will be changed:\n",
            "  libcudnn8\n",
            "The following packages will be upgraded:\n",
            "  libcudnn8 libcudnn8-dev\n",
            "2 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n",
            "E: Held packages were changed and -y was used without --allow-change-held-packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "g++ is already the newest version (4:11.2.0-1ubuntu1).\n",
            "g++ set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpdOeiJVIUT4",
        "outputId": "49931982-41dd-484b-9bdc-ab743c91f45c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matmul.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile matmul.cu\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <math.h>\n",
        "#include <stdio.h>\n",
        "#include <time.h>\n",
        "#include <iostream>\n",
        "#include <stdlib.h>\n",
        "#include \"GpuTimer.h\"\n",
        "using namespace std;\n",
        "\n",
        "#define BLOCK_SIZE 16\n",
        "#define TILE_WIDTH 16\n",
        "\n",
        "void matMulCPU(float* A, float* B, float* C, int numARows, int numACols, int numBCols) {\n",
        "    int i, j, k;\n",
        "    int offsetA, offsetB;\n",
        "    float cumSum;\n",
        "\n",
        "    for (i = 0; i < numARows; i++) {\n",
        "        for (j = 0; j < numBCols; j++) {\n",
        "            cumSum = 0;\n",
        "            for (k = 0; k < numACols; k++) {\n",
        "                // linearize index\n",
        "                offsetA = i*numACols + k;\n",
        "                offsetB = k*numBCols + j;\n",
        "\n",
        "                // accumulate element-wise product\n",
        "                cumSum += A[offsetA] * B[offsetB];\n",
        "            }\n",
        "            C[i*numBCols + j] = cumSum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void matMulGPU(float* A, float* B, float* C, int numARows, int numACols, int numBCols) {\n",
        "    // allocate shared memory\n",
        "    __shared__ float sharedA[TILE_WIDTH][TILE_WIDTH];\n",
        "    __shared__ float sharedB[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "    int bx = blockIdx.x; int by = blockIdx.y;\n",
        "    int tx = threadIdx.x; int ty = threadIdx.y;\n",
        "\n",
        "    // coordinates for C\n",
        "    int row = by * TILE_WIDTH + ty;\n",
        "    int col = bx * TILE_WIDTH + tx;\n",
        "\n",
        "    float cumSum = 0;\n",
        "    for (int m = 0; m < ceil(numACols/(float)TILE_WIDTH); m++) {\n",
        "        // load tiles\n",
        "        if ((row < numARows) && ((m*TILE_WIDTH + tx) < numACols))\n",
        "            sharedA[ty][tx] = A[row*numACols + m*TILE_WIDTH + tx];\n",
        "        else\n",
        "            sharedA[ty][tx] = 0;\n",
        "        if ((col < numBCols) && ((m*TILE_WIDTH + ty) < numACols))\n",
        "            sharedB[ty][tx] = B[(m*TILE_WIDTH + ty)*numBCols + col];\n",
        "        else\n",
        "            sharedB[ty][tx] = 0;\n",
        "        // pause until all threads have loaded tile values\n",
        "        __syncthreads();\n",
        "\n",
        "        // compute partial dot product (for individual thread)\n",
        "        for (int k = 0; k < TILE_WIDTH; k++) {\n",
        "            cumSum += sharedA[ty][k] * sharedB[k][tx];\n",
        "        }\n",
        "        // wait until all threads have used tile values\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if((row < numACols) && (col < numBCols)) {\n",
        "        C[row*numBCols + col] = cumSum;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "    int numARows = 960;\n",
        "    int numACols = 640;\n",
        "    int numBCols = 800;\n",
        "\n",
        "    if (argc == 4) {\n",
        "        numARows = std::stoi(argv[1]);\n",
        "        numACols = std::stoi(argv[2]);\n",
        "        numBCols = std::stoi(argv[3]);\n",
        "    }\n",
        "\n",
        "    cout<<\"Size of matrix A: \"<<numARows<<\" \"<<numACols<<endl;\n",
        "    cout<<\"Size of matrix B: \"<<numACols<<\" \"<<numBCols<<endl;\n",
        "\n",
        "    // timers\n",
        "    GpuTimer timer0, timer1, timer2, timer3;\n",
        "\n",
        "    size_t sizeA = numARows * numACols * sizeof(float);\n",
        "    size_t sizeB = numACols * numBCols * sizeof(float);\n",
        "    size_t sizeC = numARows * numBCols * sizeof(float);\n",
        "\n",
        "    // allocate host memory\n",
        "    float* h_A = (float*)malloc(sizeA);\n",
        "    float* h_B = (float*)malloc(sizeB);\n",
        "    float* h_C = (float*)malloc(sizeC);\n",
        "    float* h_C_CPU = (float*)malloc(sizeC);\n",
        "\n",
        "    // initialize host matrices\n",
        "    int i, j, offset;\n",
        "    for (i = 0; i <  numARows; i++) {\n",
        "        for (j = 0; j < numACols; j++) {\n",
        "            offset = i*numACols + j;\n",
        "            h_A[offset] = sin(i);\n",
        "        }\n",
        "    }\n",
        "    for (i = 0; i <  numACols; i++) {\n",
        "        for (j = 0; j < numBCols; j++) {\n",
        "            offset = i*numBCols + j;\n",
        "            h_B[offset] = cos(j);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // allocate device matrices\n",
        "    float* d_A;\n",
        "    float* d_B;\n",
        "    float* d_C;\n",
        "    timer0.Start();\n",
        "    cudaMalloc((void**)&d_A, sizeA);\n",
        "    cudaMalloc((void**)&d_B, sizeB);\n",
        "    cudaMalloc((void**)&d_C, sizeC);\n",
        "    timer0.Stop();\n",
        "    printf(\"Time to allocate memory on the device is: %f msecs.\\n\", timer0.Elapsed());\n",
        "\n",
        "    // transfer to GPU\n",
        "    timer1.Start();\n",
        "    cudaMemcpy(d_A, h_A, sizeA, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, sizeB, cudaMemcpyHostToDevice);\n",
        "    timer1.Stop();\n",
        "    printf(\"Time to copy the Matrix from host to device is: %f msecs.\\n\", timer1.Elapsed());\n",
        "\n",
        "    // kernel launch\n",
        "    dim3 threadPerBlock(BLOCK_SIZE, BLOCK_SIZE, 1);\n",
        "    dim3 blockPerGrid(ceil(numBCols/(float)BLOCK_SIZE), ceil(numACols/(float)BLOCK_SIZE), 1);\n",
        "    timer2.Start();\n",
        "    matMulGPU<<<blockPerGrid, threadPerBlock>>>(d_A, d_B, d_C, numARows, numACols, numBCols);\n",
        "    timer2.Stop();\n",
        "    printf(\"Implemented CUDA code ran in: %f msecs.\\n\", timer2.Elapsed());\n",
        "\n",
        "    // transfer to CPU\n",
        "    timer3.Start();\n",
        "    cudaMemcpy(h_C, d_C, sizeC, cudaMemcpyDeviceToHost);\n",
        "    timer3.Stop();\n",
        "    printf(\"Time to copy the resulting Matrix from device to host is: %f msecs.\\n\", timer3.Elapsed());\n",
        "\n",
        "    clock_t begin = clock();\n",
        "    matMulCPU(h_A, h_B, h_C_CPU, numARows, numACols, numBCols);\n",
        "    clock_t end = clock();\n",
        "    double time_spent = (double)(end - begin) / CLOCKS_PER_SEC * 1000;\n",
        "    printf(\"Implemented CPU code ran in: %f msecs.\\n\", time_spent);\n",
        "\n",
        "    free(h_A); free(h_B); free(h_C); free(h_C_CPU);\n",
        "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile matmulTiled.cu\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <math.h>\n",
        "#include <stdio.h>\n",
        "#include <time.h>\n",
        "#include <iostream>\n",
        "#include <stdlib.h>\n",
        "#include \"GpuTimer.h\"\n",
        "using namespace std;\n",
        "\n",
        "#define BLOCK_SIZE 16\n",
        "#define TILE_WIDTH 16\n",
        "\n",
        "void matMulCPU(float* A, float* B, float* C, int numARows, int numACols, int numBCols) {\n",
        "    int i, j, k;\n",
        "    int offsetA, offsetB;\n",
        "    float cumSum;\n",
        "\n",
        "    for (i = 0; i < numARows; i++) {\n",
        "        for (j = 0; j < numBCols; j++) {\n",
        "            cumSum = 0;\n",
        "            for (k = 0; k < numACols; k++) {\n",
        "                // linearize index\n",
        "                offsetA = i*numACols + k;\n",
        "                offsetB = k*numBCols + j;\n",
        "\n",
        "                // accumulate element-wise product\n",
        "                cumSum += A[offsetA] * B[offsetB];\n",
        "            }\n",
        "            C[i*numBCols + j] = cumSum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void matMulGPU(float* A, float* B, float* C, int numARows, int numACols, int numBCols) {\n",
        "    // allocate shared memory\n",
        "    __shared__ float sharedA[TILE_WIDTH][TILE_WIDTH];\n",
        "    __shared__ float sharedB[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "    int bx = blockIdx.x; int by = blockIdx.y;\n",
        "    int tx = threadIdx.x; int ty = threadIdx.y;\n",
        "\n",
        "    // coordinates for C\n",
        "    int row = by * TILE_WIDTH + ty;\n",
        "    int col = bx * TILE_WIDTH + tx;\n",
        "\n",
        "    float cumSum = 0;\n",
        "    for (int m = 0; m < ceil(numACols/(float)TILE_WIDTH); m++) {\n",
        "        // load tiles\n",
        "        if ((row < numARows) && ((m*TILE_WIDTH + tx) < numACols))\n",
        "            sharedA[ty][tx] = A[row*numACols + m*TILE_WIDTH + tx];\n",
        "        else\n",
        "            sharedA[ty][tx] = 0;\n",
        "        if ((col < numBCols) && ((m*TILE_WIDTH + ty) < numACols))\n",
        "            sharedB[ty][tx] = B[(m*TILE_WIDTH + ty)*numBCols + col];\n",
        "        else\n",
        "            sharedB[ty][tx] = 0;\n",
        "        // pause until all threads have loaded tile values\n",
        "        __syncthreads();\n",
        "\n",
        "        // compute partial dot product (for individual thread)\n",
        "        for (int k = 0; k < TILE_WIDTH; k++) {\n",
        "            cumSum += sharedA[ty][k] * sharedB[k][tx];\n",
        "        }\n",
        "        // wait until all threads have used tile values\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if((row < numACols) && (col < numBCols)) {\n",
        "        C[row*numBCols + col] = cumSum;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "    int numARows = 960;\n",
        "    int numACols = 640;\n",
        "    int numBCols = 800;\n",
        "\n",
        "    if (argc == 4) {\n",
        "        numARows = std::stoi(argv[1]);\n",
        "        numACols = std::stoi(argv[2]);\n",
        "        numBCols = std::stoi(argv[3]);\n",
        "    }\n",
        "\n",
        "    cout<<\"Size of matrix A: \"<<numARows<<\" \"<<numACols<<endl;\n",
        "    cout<<\"Size of matrix B: \"<<numACols<<\" \"<<numBCols<<endl;\n",
        "\n",
        "    // timers\n",
        "    GpuTimer timer0, timer1, timer2, timer3;\n",
        "\n",
        "    size_t sizeA = numARows * numACols * sizeof(float);\n",
        "    size_t sizeB = numACols * numBCols * sizeof(float);\n",
        "    size_t sizeC = numARows * numBCols * sizeof(float);\n",
        "\n",
        "    // allocate host memory\n",
        "    float* h_A = (float*)malloc(sizeA);\n",
        "    float* h_B = (float*)malloc(sizeB);\n",
        "    float* h_C = (float*)malloc(sizeC);\n",
        "    float* h_C_CPU = (float*)malloc(sizeC);\n",
        "\n",
        "    // initialize host matrices\n",
        "    int i, j, offset;\n",
        "    for (i = 0; i <  numARows; i++) {\n",
        "        for (j = 0; j < numACols; j++) {\n",
        "            offset = i*numACols + j;\n",
        "            h_A[offset] = sin(i);\n",
        "        }\n",
        "    }\n",
        "    for (i = 0; i <  numACols; i++) {\n",
        "        for (j = 0; j < numBCols; j++) {\n",
        "            offset = i*numBCols + j;\n",
        "            h_B[offset] = cos(j);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // allocate device matrices\n",
        "    float* d_A;\n",
        "    float* d_B;\n",
        "    float* d_C;\n",
        "    timer0.Start();\n",
        "    cudaMalloc((void**)&d_A, sizeA);\n",
        "    cudaMalloc((void**)&d_B, sizeB);\n",
        "    cudaMalloc((void**)&d_C, sizeC);\n",
        "    timer0.Stop();\n",
        "    printf(\"Time to allocate memory on the device is: %f msecs.\\n\", timer0.Elapsed());\n",
        "\n",
        "    // transfer to GPU\n",
        "    timer1.Start();\n",
        "    cudaMemcpy(d_A, h_A, sizeA, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, sizeB, cudaMemcpyHostToDevice);\n",
        "    timer1.Stop();\n",
        "    printf(\"Time to copy the Matrix from host to device is: %f msecs.\\n\", timer1.Elapsed());\n",
        "\n",
        "    // kernel launch\n",
        "    dim3 threadPerBlock(BLOCK_SIZE, BLOCK_SIZE, 1);\n",
        "    dim3 blockPerGrid(ceil(numBCols/(float)BLOCK_SIZE), ceil(numACols/(float)BLOCK_SIZE), 1);\n",
        "    timer2.Start();\n",
        "    matMulGPU<<<blockPerGrid, threadPerBlock>>>(d_A, d_B, d_C, numARows, numACols, numBCols);\n",
        "    timer2.Stop();\n",
        "    printf(\"Implemented CUDA code ran in: %f msecs.\\n\", timer2.Elapsed());\n",
        "\n",
        "    // transfer to CPU\n",
        "    timer3.Start();\n",
        "    cudaMemcpy(h_C, d_C, sizeC, cudaMemcpyDeviceToHost);\n",
        "    timer3.Stop();\n",
        "    printf(\"Time to copy the resulting Matrix from device to host is: %f msecs.\\n\", timer3.Elapsed());\n",
        "\n",
        "    clock_t begin = clock();\n",
        "    matMulCPU(h_A, h_B, h_C_CPU, numARows, numACols, numBCols);\n",
        "    clock_t end = clock();\n",
        "    double time_spent = (double)(end - begin) / CLOCKS_PER_SEC * 1000;\n",
        "    printf(\"Implemented CPU code ran in: %f msecs.\\n\", time_spent);\n",
        "\n",
        "    free(h_A); free(h_B); free(h_C); free(h_C_CPU);\n",
        "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nringthfIZEC",
        "outputId": "3577525a-2cfd-4417-f6c0-1b6fa05fa991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matmulTiled.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Archivo para tomar tiempos\n",
        "%%writefile GpuTimer.h\n",
        "#ifndef __GPU_TIMER_H__\n",
        "#define __GPU_TIMER_H__\n",
        "\n",
        "struct GpuTimer{\n",
        "      cudaEvent_t start;\n",
        "      cudaEvent_t stop;\n",
        "\n",
        "      GpuTimer(){\n",
        "            cudaEventCreate(&start);\n",
        "            cudaEventCreate(&stop);\n",
        "      }\n",
        "\n",
        "      ~GpuTimer(){\n",
        "            cudaEventDestroy(start);\n",
        "            cudaEventDestroy(stop);\n",
        "      }\n",
        "\n",
        "      void Start(){cudaEventRecord(start, 0);}\n",
        "\n",
        "      void Stop(){cudaEventRecord(stop, 0);}\n",
        "\n",
        "      float Elapsed(){\n",
        "            float elapsed;\n",
        "            cudaEventSynchronize(stop);\n",
        "            cudaEventElapsedTime(&elapsed, start, stop);\n",
        "            return elapsed;\n",
        "      }\n",
        "};\n",
        "\n",
        "#endif  /* __GPU_TIMER_H__ */"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTYVFgI_Ihrh",
        "outputId": "ffc2486f-d5e5-4ac8-82f7-b179609613b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting GpuTimer.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc matmul.cu -o matmul"
      ],
      "metadata": {
        "id": "0wbSFFhGIyEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matmul"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7s880LyI0Ap",
        "outputId": "9d9a6c22-017d-4fce-f0ca-971f3ad43da3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of matrix A: 960 640\n",
            "Size of matrix B: 640 800\n",
            "Time to allocate memory on the device is: 0.355456 msecs.\n",
            "Time to copy the Matrix from host to device is: 2.099616 msecs.\n",
            "Implemented CUDA code ran in: 2.812192 msecs.\n",
            "Time to copy the resulting Matrix from device to host is: 2.100320 msecs.\n",
            "Implemented CPU code ran in: 2316.234000 msecs.\n",
            "Done"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./matmul 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MKhnc4MLynC",
        "outputId": "58c5d66f-0d51-4aa2-f405-f84d08d2d617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of matrix A: 960 640\n",
            "Size of matrix B: 640 800\n",
            "Time to allocate memory on the device is: 0.297056 msecs.\n",
            "Time to copy the Matrix from host to device is: 1.110496 msecs.\n",
            "Implemented CUDA code ran in: 1.792288 msecs.\n",
            "Time to copy the resulting Matrix from device to host is: 2.076576 msecs.\n",
            "Implemented CPU code ran in: 2354.113000 msecs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./matmul 32\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBAgWO6IMQp0",
        "outputId": "183b978b-5c14-4368-c735-28ff2a17801b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of matrix A: 960 640\n",
            "Size of matrix B: 640 800\n",
            "Time to allocate memory on the device is: 0.286240 msecs.\n",
            "Time to copy the Matrix from host to device is: 1.151712 msecs.\n",
            "Implemented CUDA code ran in: 1.781952 msecs.\n",
            "Time to copy the resulting Matrix from device to host is: 2.130816 msecs.\n",
            "Implemented CPU code ran in: 2346.194000 msecs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc matmulTiled.cu -o matmulTiled"
      ],
      "metadata": {
        "id": "FjONkNamMYOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matmulTiled 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8VXE0uSMaLL",
        "outputId": "2297a019-ebd7-4cb7-e881-d148a42bc006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of matrix A: 960 640\n",
            "Size of matrix B: 640 800\n",
            "Time to allocate memory on the device is: 0.254336 msecs.\n",
            "Time to copy the Matrix from host to device is: 1.120736 msecs.\n",
            "Implemented CUDA code ran in: 1.786208 msecs.\n",
            "Time to copy the resulting Matrix from device to host is: 2.080736 msecs.\n",
            "Implemented CPU code ran in: 2358.828000 msecs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./matmulTiled 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMJVUOxbMb4b",
        "outputId": "d8a1b52f-66e3-4272-a0fa-2fbeeb977f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of matrix A: 960 640\n",
            "Size of matrix B: 640 800\n",
            "Time to allocate memory on the device is: 0.370944 msecs.\n",
            "Time to copy the Matrix from host to device is: 1.135648 msecs.\n",
            "Implemented CUDA code ran in: 1.781856 msecs.\n",
            "Time to copy the resulting Matrix from device to host is: 2.654464 msecs.\n",
            "Implemented CPU code ran in: 2812.543000 msecs.\n"
          ]
        }
      ]
    }
  ]
}